{
  "content": [
    {
      "date": "2016-06-22",
      "summary": "Some language agnostic good practices from Google on how to make code long-lived. They talk about the usual suspects: consistency, style-guides focusing on code patterns, code reviews, encapsulating third-party dependencies, etc. Some of the less common advice includes setting up champion roles for domains like security or readability who have the explicit responsibility of mentoring others in their domain and therewith act as a force-multiplier for good.",
      "title": "Lessons in Sustainability: How to Maintain a C++ Codebase for Decades",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=zW-i9eVGU_k"
    },
    {
      "date": "2016-06-21",
      "summary": "In order to come up with descriptive names, engineers often end up with very long variable names. The article claims that overly long names are bad because they harm the flow of the code and lead to misleading precision. Strategies to avoid long names include removing words that are obvious from the variable's type or from the class context and removing words that don't add to the disambiguation of the variable.",
      "title": "Long Names Are Long",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://journal.stuffwithstuff.com/2016/06/16/long-names-are-long/"
    },
    {
      "date": "2016-06-21",
      "summary": "Concise summary of the key benefits of immutable objects: thread safe, validate-once, no need to reason about state, etc. Good reference article.",
      "title": "Immutable objects",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://www.javapractices.com/topic/TopicAction.do?Id=29"
    },
    {
      "date": "2016-06-20",
      "summary": "Real-world usage example of SQLite which confirms that it is a good choice for read-dense, write-sparse applications that don't have many concurrent writes.",
      "title": "We're pretty happy with SQLite",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://beets.io/blog/sqlite-performance.html"
    },
    {
      "date": "2016-06-20",
      "summary": "Summary of some research on tree-based classifiers: boosted trees may outperform random forest for relatively low-dimensional problems but the increase in performance comes with the downside of having many more parameters to tune.",
      "title": "What is better: gradient-boosted trees, or a random forest?",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/"
    },
    {
      "date": "2016-05-25",
      "summary": "Good deconstruction of the issues with using NodeJS for larger projects. The ecosystem of libraries and tools moves too quickly without adding benefits. Error handling and concurrency are difficult. There is a general lack of stability and standards in the community.",
      "title": "After a year of using nodejs in production",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://geekforbrains.com/post/after-a-year-of-nodejs-in-production"
    },
    {
      "date": "2016-05-25",
      "summary": "Great evaluation of the clustering algorithms in scikit-learn. K-Means, Mean Shift and Affinity Propagation have strong spherical assumptions so they are not great for most data-sets. Spectral clustering and agglomerative clustering need to assign all data points to clusters so the results tend to be too noisy. DbScan produces good clusters but has a low recall. H-DbScan improves the recall of DbScan, can be implemented really efficiently and should thus be the default choice for general-purpose clustering tasks.",
      "title": "Comparing Python Clustering Algorithms",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://nbviewer.jupyter.org/github/lmcinnes/hdbscan/blob/master/notebooks/Comparing%20Clustering%20Algorithms.ipynb"
    },
    {
      "date": "2016-05-25",
      "summary": "A great primer on the ideals and practices behind extreme programming. Gives an especially good explanation of what puts the extreme into extreme programming: taking existing software development ideas and taking them to their logical conclusion.",
      "title": "eXtreme Programming (XP) for beginners",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=RUlsNnzFWy8"
    },
    {
      "date": "2016-05-18",
      "summary": "The paper presents a small study where they find further backup for the hypothesis that pair programming leads to better code in terms of maintainability and defect rate. They then analyze when pair programming is more economically viable than solo programming, regardless of software quality. They find that pairing is efficient if the pair doesn't have experience with the task at hand. Efficiency rapidly drops as familiarity increases. This means that pair programming is more useful for junior engineers than senior engineers.",
      "title": "Pair programming productivity: Novice-novice vs. expert-expert",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://www.cs.utexas.edu/users/mckinley/305j/pair-hcs-2006.pdf"
    },
    {
      "date": "2016-05-08",
      "summary": "Based on a small study with students, the paper finds that pair programming is not significantly more expensive than individual contributors with code review.",
      "title": "Are Reviews an Alternative to Pair Programming?",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://www.ipd.uka.de/Tichy/uploads/publikationen/77/MllerPairReviewsESE2004.pdf"
    },
    {
      "date": "2016-05-08",
      "summary": "The authors run a small study with students. They find that pair programming with remote teams has similar effects than when the teams are collocated. They also find that remote pair programming is a significant contributor to team cohesion and communication.",
      "title": "Exploring the efficacy of distributed pair programming",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://webguide.cs.colorado.edu:3232/dlc-2004/uploads/154/XPU2002DXP.pdf"
    },
    {
      "date": "2016-05-08",
      "summary": "The paper draws from studies and anecdotes to formulate some ground rules for effective pair programming. Most of the suggestions focus around ego-less programming, collective ownership, taking a break from each other every now and then, and creating an atmosphere where the programming pair has mutual respect and mutual responsibility.",
      "title": "All I Really Need to Know about Pair Programming I Learned In Kindergarten",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://www2.york.psu.edu/~sg3/cmpbd205/assign/week01/ACMarticlePairProgramming.pdf"
    },
    {
      "date": "2016-05-08",
      "summary": "The authors conduct an experiment with students, comparing the effect of software methodology on various factors like code complexity, defect rate and time-to-completion. Their most interesting finding is that pair programming leads to the lowest standard deviation across participants in all metrics: pair programming is thus a step on the way towards predictable software delivery.",
      "title": "Experimental Evaluation of Pair Programming",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.1689&rep=rep1&type=pdf"
    },
    {
      "date": "2016-05-08",
      "summary": "The paper analyzes 18 studies on pair programming and finds that overall pairing is beneficial on complex tasks where defect rate is lowered and on simple tasks where time-to-completion is reduced. The study also finds that pair programming is a great way to make use of junior engineers as it raises their performance to near-senior level.",
      "title": "The effectiveness of pair programming: A meta-analysis",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://www.idi.ntnu.no/grupper/su/publ/ebse/R11-pairprog-hannay-ist09.pdf"
    },
    {
      "date": "2016-05-08",
      "summary": "The authors ran an experiment on pair programming using students. They find that pairing decreases the time-to-delivery of a feature by at least 40% but increases the total man-hours spent on the feature by up to 60%. They also observe that once the pairs get more acquainted and start to gel, this overhead drops to as low as 15%.",
      "title": "Strengthening the Case for Pair-Programming",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://www.cs.utah.edu/~lwilliam/Papers/ieeeSoftware.PDF"
    },
    {
      "date": "2016-05-07",
      "summary": "This is a great book about writing software in general, the Java-specifics are few and far between and can easily be translated into other languages. The book covers many high-level concerns from process to style, from testability to culture, offering a fairly comprehensive overview of the things to look out for in a modern software project. A must-read for any fledgling software developer, especially due to its short length.",
      "title": "Java for small teams",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "https://ncrcoe.gitbooks.io/java-for-small-teams/"
    },
    {
      "date": "2016-05-05",
      "summary": "The paper proposes some ways to improve the Naive Bayes classifier for text data. They propose two heuristics to improve performance. Scaling word frequency counts using a log transform adjusts the multinomial distribution inferred by Naive Bayes to better match the power law distribution normally seen with text data. Scaling the frequency counts using inverse document frequency and using a document-length term increases performance further. They also propose two conceptual improvements to Naive Bayes. Normalizing weight vectors prevents non-independent classes from getting too much importance. Training Naive Bayes on the complement of the data-set for each class prevents over-represented classes from affecting the parameter estimate.",
      "title": "Tackling the Poor Assumptions of Naive Bayes Text Classifiers",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf"
    },
    {
      "date": "2016-04-24",
      "summary": "Surprisingly good overview of some of the most widely used patterns like decorator, factory, strategy, facade, observer, and so forth. Gives many different ways to understand the material in order to reinforce the learning; conversational style, pro-and-cons lists, diagrams, stories. Nothing overly deep, but definitely more digestible as a first introduction to patterns than the Gang of Four book.",
      "title": "Head First Design Patterns",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "https://www.amazon.co.uk/dp/B00AA36RZY/"
    },
    {
      "date": "2016-04-23",
      "summary": "The article contains a quite complete checklist of things to look out for in code reviews, covering design, testability and style. More interestingly, the article also provides some advice for how to establish a successful code review culture. The submitter should self-review their changes first. Reviewers should tag suggestions with a notion of importance for prioritization, explain the rationale behind suggestions for education and do their best to criticize the code, not the author. The article also makes a good point around creating a culture where code reviews don't only point out the bad but also compliment the good in order to create positive reinforcement and sharing of good practices.",
      "title": "Code Review Best Practices",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://kevinlondon.com/2015/05/05/code-review-best-practices.html"
    },
    {
      "date": "2016-04-22",
      "summary": "Cautionary tale of a failed software rewrite. The cause: classics. The rewrite did not add any customer benefits, maintenance and new features on the existing code caused deadlines to slip, opportunity cost, etc. The article also gives some recommendations for when a rewrite might be warranted that mostly focus around tight coupling and individual refactors touching too much code.",
      "title": "When to Rewrite from Scratch - Autopsy of a Failed Software",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://codeahoy.com/2016/04/21/when-to-rewrite-from-scratch-autopsy-of-a-failed-software/"
    },
    {
      "date": "2016-04-13",
      "summary": "Great list of common warning signs in code that indicate the opportunity for a refactor. Covers smells in the implementation of a single class and in the interaction between classes.",
      "title": "Code Smells",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://blog.codinghorror.com/code-smells/"
    },
    {
      "date": "2016-04-09",
      "summary": "A scikit-learn style library for feature selection. Contains lots of algorithms both for supervised an unsupervised feature selection. Nice touch: the documentation provides references to papers that explain how the algorithms work and when they should be used.",
      "title": "An brief introduction on how to perform feature selection with scikit-feature",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://featureselection.asu.edu/tutorial.php"
    },
    {
      "date": "2016-04-06",
      "summary": "Mostly straight forward advice about code reviewing: don't hesitate to review a change even if you aren't an expert in the area, review your code before submitting it for wider review, use check-lists, use automation, etc. A nice but less commonly stressed point: use positive reinforcement and also comment on things that are done well.",
      "title": "Yet Another Code Review Best Practices",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "https://www.future-processing.pl/blog/another-code-review-best-practices/"
    },
    {
      "date": "2016-04-05",
      "summary": "The CMMI process recommends to keep a short personal code review checklist that prompts the developer to code review their own code for common mistakes that they make pre submission for wider review. This list can be mined from peer review comments and should evolve over time.",
      "title": "Personal Code Reviews",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "https://helloscriptkitty.wordpress.com/2010/03/03/personal-code-reviews/"
    },
    {
      "date": "2016-04-03",
      "summary": "Demonstration of how easy it is to combine Spark and Scikit for simple parallelization of the more advanced algorithms that Scikit provides but that don't exist in ML-Lib.",
      "title": "Integrating Spark with Scikit-Learn",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "https://adventuresindatascience.wordpress.com/2016/04/02/integrating-spark-with-scikit-learn-visualizing-eigenvectors-and-fun/"
    },
    {
      "date": "2016-03-30",
          "summary": "A short piece that focuses on how to implement one of the main things that makes an engineer senior: teaching others. Unsurprisingly, pair programming gets a mention. On a more subtle note, the article also mentions that great mentors should be on the look-out for systemic issues and people with problems to proactively offer help.",
      "title": "On Engineering Mentorship",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "https://medium.com/@justinbean/on-engineering-mentorship-7fddd187cf3e"
    },
    {
      "date": "2016-03-29",
      "summary": "Three main pieces of advice on how to avoid legacy code becoming too much of a pain. Constantly upgrade dependencies in order to avoid big migrations that are sure to be expensive and need to be factored into a sprint as opposed to small upgrades that just come out in the wash of the day to day. Prioritize execution speed of unit tests in order to encourage developers to add to the test suite before making a change and to run tests regularly while developing. Having an infinite backlog makes it easy to ignore technical debt from the management side so once a development team claims that a particular cause of debt slowed development of a feature down more than some number of times, it's their responsibility to push for those debt reduction tasks to get prioritized.",
      "title": " How to Avoid Brittle Code",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "https://www.go.cd/2016/03/24/how-to-avoid-brittle-code.html"
    },
    {
      "date": "2016-03-29",
      "summary": "Quick overview of the most important ingredients for clean code, maybe to be read as a prelude to Code Complete. Make code look the same by avoiding to slip-in personal quirks or preferences that ultimately don't matter like Yoda conditionals. Apply the single responsibility principle and aim to keep the level of abstraction consistent across a function. Reduce the element of surprise by keeping the architecture straight forward, use meaningful names and avoid using new hard to understand technology.",
      "title": "How to reduce the cognitive load of your code",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://chrismm.com/blog/how-to-reduce-the-cognitive-load-of-your-code/"
    },
    {
      "date": "2016-03-28",
      "summary": "A bit self-help oriented and somewhat short on practical content. The writing is straight-forward and somewhat entertaining to read, making at least the first section on career development worth a browse.",
      "title": "Soft Skills: The Software Developer's Life Manual",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/1617292397/"
    },
    {
      "date": "2016-03-28",
      "summary": "The author criticizes three aspects of the scrum methodology. Planning poker too easily devolves into the opinion of the highest importance person with little to no discussion to find a consensus estimate. Velocity is too easily abused for micro management and for setting delivery targets. Standups get out of hand and are an unproductive use of face to face time. These points all ring true but are more criticisms of a scrum implementation rather than attacking the methodology at its core and could easily be mitigated, for instance by openly encouraging discord during planning poker, avoiding to use velocity as anything but a sprint-size estimation tool and by holding standups asynchronously.",
      "title": "Why Scrum Should Basically Just Die In A Fire",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://gilesbowkett.blogspot.ca/2014/09/why-scrum-should-basically-just-die-in.html"
    },
    {
      "date": "2016-02-25",
      "summary": "This is the baseline presented in the Duan and Hsu paper. In essence, they have a very fast and memory-efficient trie-based way to look up strings that are approximately prefix equivalent. This algorithm isn't an entirely fair baseline for the Duan and Hsu paper because in order to work in a general search engine context, the algorithm would require the entire corpus of the search engine or of all acceptable search queries to be in memory.",
      "title": "Extending Autocompletion To Tolerate Errors",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://research.microsoft.com/pubs/80005/sigmod513.pdf"
    },
    {
      "date": "2016-02-25",
      "summary": "The paper presents a quite sophisticated spelling correction model based on a Markov model that encodes the probability of transforming a sequence of misspelled word chunks into a sequence of word chunks of a correct query. The most surprising thing about the paper is the great performance of the baseline against which they compare their model. They suggest an evaluation using the 'minimal keystrokes' metric which counts the number of keystrokes that a user has to enter before being able to get to the correctly spelled query. They find that their model outperforms the baseline, but only by about two keystrokes and a few percentage points of recall and precision. The baseline is based on simple edit-distance.",
      "title": "Online Spelling Correction for Query Completion",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://research.microsoft.com/pubs/148103/www11-onlinespellingcorrection.pdf"
    },
    {
      "date": "2016-02-23",
      "summary": "Short overview of the code review process of one of the Mozilla core developers. Great to see a strong focus on reviewing the commit message and the high-level changes such as public method signatures instead of focusing only on the implementation itself.",
      "title": "How I Do Code Reviews at Mozilla",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://benjamin.smedbergs.us/blog/2014-10-22/how-i-do-code-reviews-at-mozilla"
    },
    {
      "date": "2016-02-23",
      "summary": "Interesting article that summarizes a number papers on quantitative analysis of software development methodologies published by Microsoft Research and thus provides data to back up some best practices in the industry. For example they provide evidence for using TDD and assertion-dense code: they found that TDD reduces the number of bugs by 60-90% and that assertion density has a significant correlation with fewer bugs. The cost: TDD increases development time by 15-35%. They also provide evidence for distributed teams: they didn't find a significant correlation between a team's geographic dispersal and increased software defects.",
      "title": "Exploding Software-Engineering Myths",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://research.microsoft.com/en-us/news/features/nagappan-100609.aspx"
    },
    {
      "date": "2016-02-15",
      "summary": "They propose an extension of the n-gram spelling correction model: multiply the language model probability by the probability of observing a n-character spelling correction. Their extension outperforms the stand-alone n-gram language model approach. Another take-away is how they automatically generate spelling correction training data from search engine query logs. They take all sessions where the user searches for something and then for another thing straight thereafter without any other interaction. If the two queries are similar in terms of edit distance, the second query likely is a correction of the first. In order to increase the yield from the technique, they also extend the candidate corrections using n-gram expansion.",
      "title": "Spelling Correction Based on User Search Contextual Analysis and Domain Knowledge",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://maroo.cs.umass.edu/getpdf.php?id=878"
    },
    {
      "date": "2016-02-13",
      "summary": "Some well-intentioned wise words around what to expect as a open source maintainer - or rather, what to not expect: don't expect praise, don't expect people to be nice, etc. The points around user engagement were useful: use animated gifs if you want people to understand something, visual demos are more effective than plans.",
      "title": "Advice To Open Source Project Contributors",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://taskwarrior.org/docs/advice.html"
    },
    {
      "date": "2016-02-13",
      "summary": "The paper claims that trigram-based real-word spelling correction systems have a performance bottle-neck: data sparsity. In order to address this issue, they propose a mixed approach, incorporating part of speech information to augment the available trigrams. For example, for a source trigram like the/det mouse/noun eats/verb, they would generate the mixed trigrams det-noun-verb, the-noun-verb, det-mouse-verb, and so forth. They then use a Markov Model to perform the spelling correction. Unfortunately, they don't compare their model to a standard model like MDM. They note that their approach has a relatively high recall but also a high false positive rate which may be linked to the part-of-speech tags not having enough discriminative power. Perhaps a simpler and more robust approach to dealing with trigram data sparsity would be using bigrams or some sort of smoothing and interpolation.",
      "title": "A Mixed Trigrams Approach for Context Sensitive Spell Checking",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://nlp.cs.uic.edu/PS-papers/spell-cicling07.pdf"
    },
    {
      "date": "2016-02-13",
      "summary": "An apology-by-example to the Hungarian notation: misunderstood and more useful than many believe. Contrary to popular opinion, the original intent of the Hungarian notation was not to encode a variable type, such as integer or string, but instead to encode a variable intent, such as width or unsafe-user-input. As such, the Hungarian notation is actually pretty useful since it increases collocation by permanently reminding the programmer what a variable is supposed to do without having to trace the origin of the variable. Hungarian notation also makes it obvious to spot mistakes arising from variables being misused for incorrect purposes, such as multiplying a width by an array-index. Nevertheless, the article arises the question: instead of encoding intent in the variable name, why not create a custom class?",
      "title": "Making Wrong Code Look Wrong",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://www.joelonsoftware.com/articles/Wrong.html"
    },
    {
      "date": "2016-02-10",
      "summary": "The paper reviews an old trigram-based contextual spelling correction system. The system is straight forward. First, compute the probability of the sentence under a language model. Then, replace the candidate incorrect word with all spelling alternatives and recompute the sentence probability for each alternative spelling. If the probability of the original sentence is low compared to the alternative sentences, the candidate incorrect word is a spelling mistake and is replaced with the alternative word in the alternative sentence with the highest probability under the language model. They find that this approach is quite effective at finding errors in all but closed-class words, especially when training the language model on larger corpora. The model is limited to spelling mistakes at edit distance one, but they find that this covers the vast majority of cases. Extending the model to cope with more than one mistake per word increases false positives and does not increase overall performance. The approach is pretty neat: conceptually simple and doesn't rely on labelled training data.",
      "title": "Real-word spelling correction with trigrams: A reconsideration of the Mays, Damerau, and Mercer model",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://ftp.cs.toronto.edu/pub/gh/WilcoxOHearn-etal-2006.pdf"
    },
    {
      "date": "2016-02-10",
      "summary": "The paper proposes a contextual spelling correction system based on latent semantic analysis (LSA). Their approach relies on having a training set of confusion words. One example of confusion words are homophones such as 'affect' and 'effect'. Another set of confusion words are typing mistakes that lead to correct words such as 'quiet' and 'quite'. They build an LSA space for each set of confusion words, treating each sentence in which one of the confusion words occurs as a training document. They also apply some cleanup, weighting and n-gram expansion. Given a sentence they can now predict which word in the confusion set is the correct one: replace the confusion candidate in the sentence with each word from the confusion set, turn the sentence into a vector in the LSA space and see to which confusion word in the space the sentence is closest using cosine similarity. Their approach is a bit weak: they barely outperform a trigram Naive Bayes model, they need labelled sets of training confusion words and the all-versus-all cosine similarity computation probably won't scale to large amounts of data.",
      "title": "Contextual Spelling Correction Using Latent Semantic Analysis",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://www.aclweb.org/anthology/A97-1025.pdf"
    },
    {
      "date": "2016-01-20",
      "summary": "The article talks about a common problem with legacy software and summarizes the points with a succinct tl;dr: Successive, well intentioned, changes to architecture and technology throughout the lifetime of an application can lead to a fragmented and hard to maintain code base. Sometimes it is better to favour consistent legacy technology over fragmentation. Unfortunately, the article doesn't provide any strategies how to avoid the lava layer anti-pattern, other than appealing to the developer's sense of professionalism and business value. Perhaps providing developers who work on legacy applications with a counter-balance such as a playground to work on green-field projects could help fight the lava layer?",
      "title": "The Lava Layer Anti-Pattern",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://mikehadlow.blogspot.ca/2014/12/the-lava-layer-anti-pattern.html"
    },
    {
      "date": "2016-01-18",
      "summary": "When Steve Yegge writes, it's usually worth a read. On the surface, this blog post is an entertaining rant comparing the big tech companies: Google vs Microsoft vs Amazon vs Facebook. On a second level, the post is a critique of service-oriented-architecture and full-ownership engineering teams. But in essence, the post makes the point that for successful products, one needs to have a platform; although owning a platform is hard and difficult to retro-fit.",
      "title": "Stevey's Google Platforms Rant",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "https://plus.google.com/+RipRowan/posts/eVeouesvaVX"
    },
    {
      "date": "2016-01-17",
      "summary": "The Marisa trie is a highly space-efficient read-only dictionary. Their benchmarks claim that it is about an order of magnitude more space-efficient than other deterministic dictionary implementations. Hashing-based dictionaries like bloom filters are even smaller but come with the cost of false-positives. The library is implemented in C++ with bindings for many high-level languages including Python.",
      "title": "MARISA: Matching Algorithm with Recursively Implemented StorAge",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "http://marisa-trie.googlecode.com/svn/trunk/docs/readme.en.html"
    },
    {
      "date": "2016-01-11",
      "summary": "The article states that Netflix uses an anomaly-detection system based on clustering via DBSCAN: for every performance counter, normal servers should fall within a cluster and anomalies outside of the cluster. More interestingly, the article also talks about how to drive adoption of machine learning systems by service owners. Their anomaly detection algorithm has two meta-parameters; requiring people deploying the system to tune the parameters would greatly hurt adoption. In order to address this issue, they developed an automatic tuning mechanism based on simulated annealing. They claim that the self-tuning nature of their anomaly detection algorithm was one of its main adoption-drivers.",
      "title": "Tracking down the Villains: Outlier Detection at Netflix",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://techblog.netflix.com/2015/07/tracking-down-villains-outlier.html"
    },
    {
      "date": "2015-12-24",
      "summary": "Great overview of the software development process of the Chromium project, one of the biggest and most successful open source projects out there. The article claims that the main ingredient to their rapid ship cycle is that every developer always works off of head (to avoid costly integrations) which is enabled by high test coverage (to avoid breaking things) and feature flags (to hide partial features until they ship). They also talk about relentless refactoring but fail to mention how large-scale refactors fit in with constantly working off of head: how does one check in a partially completed refactor?",
      "title": "How Chromium Works",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "https://medium.com/@aboodman/in-march-2011-i-drafted-an-article-explaining-how-the-team-responsible-for-google-chrome-ships-c479ba623a1b"
    },
    {
      "date": "2015-12-06",
      "summary": "Lots of fluff and not much substance. The majority of the book is filled with examples of how real-world companies solved some problems with data and how they went about solving the problem. The main interesting point in the book is the framework that the authors introduce to discuss data problems. The framework is useful in order to keep in mind that problem recognition, definition and communication are as important as solving the analytical issues at hand.",
      "title": "Keeping Up with the Quants: Your Guide to Understanding and Using Analytics",
      "topics": ["Management"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B00B0SA1LY/"
    },
    {
      "date": "2015-10-30",
      "summary": "One of the best hands-on books on writing good code in recent memory. Lots of practical advice on how to write testable (i.e. maintainable) code and how to safely make bad code more testable. Succinctly written and a good reference on the how-why-and-when of a variety of refactors.",
      "title": "Working Effectively with Legacy Code",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B005OYHF0A/"
    },
    {
      "date": "2015-10-15",
      "summary": "An interesting classic: thirty years old and yet it contains a lot of wisdom that is only slowly and gradually being accepted and applied (and where it is applied, usually to universal success) -- however, the book is also quite dated in many ways, relatively low on information density and therefore ultimately not an enjoyable-or-commendable read to anyone but the 'Agile historian'; more modern texts simply offer more to the practitioner.",
      "title": "The Psychology of Computer Programming",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B004R9QACC/"
    },
    {
      "date": "2015-07-26",
      "summary": "The paper proposes a standard way to approach denormalizing data that makes it easier to visualize, analyze and find outliers: avoid encoding information in column headers, avoid multi-variate data-types in columns, avoid data-types that require both column and row values to encode information, avoid multiple data-types in the same table, avoid spreading a logical row over multiple tables, etc.",
      "title": "Tidy Data",
      "topics": ["Data Science"],
      "type": "Paper",
      "url": "http://vita.had.co.nz/papers/tidy-data.pdf"
    },
    {
      "date": "2015-06-25",
      "summary": "Very high-level, easy to read, non-technical introduction to some of the main problems and gotchas in machine learning, e.g. evaluation, feature engineering, representation, etc. Probably a good text to give to family and friends who would like to know a bit more about the field. Anyone already involved in the field should already be familiar with everything mentioned in the paper and can therefore pass on it.",
      "title": "A Few Useful Things to Know about Machine Learning",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"
    },
    {
      "date": "2015-06-25",
      "summary": "Missing value ratio: remove features with high number of missing values. Low variance filter: remove features with low variance (only applies to numeric features). High correlation filter: remove correlated features using Person's Product Moment Coefficient or Chi-Squared test - remember to scale features first (doesn't work for numeric/categorical pairs of features). Trees: build a random forest on the data and see how often each feature is used in the ensemble - frequently used features are good features due to having a high information gain. Principal components: PCA destroys feature interpretability (only applies to numeric features). Backward elimination: remove one feature at a time and re-train the classifier and kill the feature with the lowest increase in error (very slow). Forward construction: start with one feature and add features to the classifier until the error decrease is small (very slow). They compare all approaches on the KDD-2009 data-set and find that Trees perform best in terms of dimensionality-reduction-to-AUC ratio but that simple methods like Missing value/high-correlation filters are not far behind.",
      "title": "Seven Techniques for Data Dimensionality Reduction",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.knime.org/blog/seven-techniques-for-data-dimensionality-reduction"
    },
    {
      "date": "2015-06-22",
      "summary": "Short rant on the over-use of map-reduce for small data-set sizes and how that puts constraints on the types of expressible computations. Alternatives: add more memory to your server and process the data in memory or stream it. If the data is really too big for memory, check if a database on an SSD will give decent performance. Map-reduce should be reserved for terabyte-scale data. Hard to disagree with the pragmatism.",
      "title": "Don't use Hadoop - your data isn't that big",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html"
    },
    {
      "date": "2015-06-22",
      "summary": "Cute cautionary tale about the importance of data-cleanup. It turns out that most of Wikipedia was written by natural language generation algorithms running on databases of facts, e.g. about small towns or rivers. A failure to remove these articles from a crawl of Wikipedia will lead to terrible training data. Work-around: only train on the head articles, i.e. the articles that get lots of human eyeballs.",
      "title": "The Unknown Perils of Mining Wikipedia",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/"
    },
    {
      "date": "2015-06-21",
      "summary": "The paper makes the argument that for most data-set sizes common in industry, using a cluster of map-reduce machines is inefficient compared to using fewer but bigger servers given recent reductions in the cost of memory. Other advantages of bigger servers over clusters are better algorithms (no distributed approximations required) and a simpler programming model. The down-side of having to load more data on a single machine can be mitigated to some extent by investing into better ethernet connections and faster disks.",
      "title": "Nobody ever got fired for using Hadoop on a cluster",
      "topics": ["Data Science"],
      "type": "Paper",
      "url": "http://research.microsoft.com/pubs/163083/hotcbp12%20final.pdf"
    },
    {
      "date": "2015-06-06",
      "summary": "The first few chapters of this book were quite interesting, likening the origins of the lean software movement to its roots in lean manufacturing: which practices made sense to port over, which were inappropriate, what ideas should we still adopt, etc. I especially liked the rather critical view taken on the dogmatic appropriation of Agile techniques - a breath of fresh air. The later parts of the book strayed into a lot heavier management territory and thus were less useful from an software engineer's point of view: team building, motivation, contracts, etc. Summary: the first half well worth reading, the latter half only for aspiring managers.",
      "title": "Lean Software Development: An Agile Toolkit",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B00HEL13HW/"
    },
    {
      "date": "2015-05-21",
      "summary": "Wrapper around Matplotlib that renders graphs to interactive Javascript. Very cool.",
      "title": "MPLD3",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "https://mpld3.github.io/index.html"
    },
    {
      "date": "2015-05-21",
      "summary": "SciKit-Learn style wrapper for some genetic algorithm libraries... more fancy stuff made easier.",
      "title": "gplearn",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "https://github.com/trevorstephens/gplearn"
    },
    {
      "date": "2015-05-21",
      "summary": "SciKit-Learn style wrapper around a bunch of standard neural network libraries... reduces the barrier to entry.",
      "title": "scikit-neuralnetwork",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "https://github.com/aigamedev/scikit-neuralnetwork"
    },
    {
      "date": "2015-05-21",
      "summary": "Very fun blog-post that walks through using RNNs to generate natural language and structured texts from character level input. Gives a lightweight background on the theory behind character-level networks and talks about some practical considerations. Unfortunately doesn't compare the RNN output to simpler models like Markov Chains or N-Gram language models to ground their performance.",
      "title": "The Unreasonable Effectiveness of Recurrent Neural Networks",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
    },
    {
      "date": "2015-04-27",
      "summary": "Classic short text presenting a three-step approach to reading academic papers efficiently. The first pass should be a very short weed-out pass: what are the conclusions, what is the impact, is it likely to be correct, is it relevant to your research. The second pass is there to find flaws in the argument (requires getting a good understanding of the main argument). If the paper is not 100% relevant to your research, stop now. Otherwise, the third pass is the deconstruction piece: what are the strong/weak points of the paper, reach the same conclusions as the paper given the same assumptions, what are good avenues for future work.",
      "title": "How to Read a Paper",
      "topics": ["Meta"],
      "type": "Paper",
      "url": "http://ccr.sigcomm.org/online/files/p83-keshavA.pdf"
    },
    {
      "date": "2015-04-26",
      "summary": "The paper makes a case for using color histograms for image representation: invariant under scale, rotation, translation, easy to compute. They use linear SVMs and SVMs with fancy kernels on top of the color histogram representation. They find that exponentiating the histogram bins (x_i := x_i^a for 0 <= a <= 1) makes linear SVMs perform almost as well as fancy SVMs.",
      "title": "SVMs for Histogram-Based Image Classification",
      "topics": ["Computer Vision"],
      "type": "Paper",
      "url": "http://olivier.chapelle.cc/pub/tnn99.pdf"
    },
    {
      "date": "2015-04-26",
      "summary": "Netflix's uses a three-step approach to page generation: first predict how a user would score a particular title, from this generate rows of content and then select which rows to show and in which order. All fairly standard stuff but for one interesting point: in order to avoid having monotonous content on the page, they score each row individually and combine it with a score relative to its neighbours to increase diversity.",
      "title": "Learning a Personalized Homepage",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://techblog.netflix.com/2015/04/learning-personalized-homepage.html"
    },
    {
      "date": "2015-04-26",
      "summary": "Comparison of some off-the-shelf sentence splitters. Stanford CoreNLP (finite-state-machine word-tokenization first, then sentence grouping) and OpenNLP (word-boundary detection as a classification problem) have very low error rates. Regular expression based approaches work less well. The analysis should be taken with a grain of salt (they only used a single source of documents, reporting point-estimates instead of cross-validation results, etc.) but is likely a good starting point when prioritizing what sort of sentence splitter to try first on one's own dataset.",
      "title": "How to Split Sentences",
      "topics": ["Natural Language Processing"],
      "type": "Article",
      "url": "http://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html"
    },
    {
      "date": "2015-04-16",
      "summary": "Lots of hype around this article so it had to be read eventually. Turns out that the piece is a surprisingly good birds-view description of the who-what-why-and-where of data science. Great article to send to non-technical folk who are curious about the field.",
      "title": "Data Scientist: The Sexiest Job of the 21st Century",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century"
    },
    {
      "date": "2015-04-16",
      "summary": "The author compares a number of different approaches to sentiment analysis. The first approach is based on bag-of-words plus Naive Bayes. The second approach is based on lexicons: words are associated with a positive or negative score and word-scores are aggregated over the entire text. (Immediate concern: the approach requires word sense disambiguation - this is a really hard problem! The author uses the standard naive WordNet-overlap approach.) The third approach uses the deep-net classifier that is part of Stanford CoreNLP. (Immediate concern: the classifier is trained on data from an entirely different domain.) The Naive Bayes approach beats out the other two, achieving about 90% accuracy. The result is somewhat unsurprising as the lexicon approach is constrained by the guaranteed poor performance of the author's word-sense disambiguation technique and the CoreNLP deep-net is trained on out-of-domain data. The text shows - once again - the power of simple methods like bag-of-words, but the results shouldn't be taken too seriously. A more interesting comparison would have re-trained the deep-net to answer the question how much a learned representation of the text improves classification performance.",
      "title": "A comparison of open source tools for sentiment analysis",
      "topics": ["Natural Language Processing"],
      "type": "Article",
      "url": "http://fotiad.is/blog/sentiment-analysis-comparison"
    },
    {
      "date": "2015-03-29",
      "summary": "Interesting take on using experiments and computational methods like randomized permutations or boostrapping to understand statistical phenomena instead of relying on difficult to understand formulae.",
      "title": "Statistics Without the Agonizing Pain",
      "topics": ["Statistics"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=5Dnw46eC-0o"
    },
    {
      "date": "2015-03-22",
      "summary": "They released an algorithm that selects the fastest nearest-neighbor search algorithm for some data-set. For most data-sets (even in high dimensions), it turns out that searching trees of hierarchical k-means clusters or randomized kd-trees work well. It is not surprising but nevertheless interesting that decomposing a hard problem in high dimensions into many smaller problems in lower dimensions works well.",
      "title": "Fast Approximate Nearest Neighbors With Automatic Algorithm Configuration",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_visapp09.pdf"
    },
    {
      "date": "2015-03-22",
      "summary": "Regression algorithm that can was designed to deal with lots of outliers; the algorithm can handle more than half of the data-set being outliers. First, we create random minimal sample sets from the data. The size of these sets should be just big enough to fit our regression e.g., two elements for a two-dimensional feature space. We then fit a regression to the minimal sample sets and check how many points in the full data-set are consistent with this regression up to some delta of error. Iterate a fixed number of times or until the consensus set is large enough. Problems: how do we determine the error threshold and minimal acceptable consensus set size?",
      "title": "RANSAC for dummies",
      "topics": ["Statistics"],
      "type": "Paper",
      "url": "http://vision.ece.ucsb.edu/~zuliani/Research/RANSAC/docs/RANSAC4Dummies.pdf"
    },
    {
      "date": "2015-03-22",
      "summary": "They train a convolutional neural network on raw text (input features are non-overlapping windows of characters). They create additional noisy training data by replacing words in their training sentences with synonyms taken out of a dictionary which helps in making the network more invariant under non-semantic changes (like adding noise in speech recognition or scaling/rotating input images in computer vision). They apply the model to a bunch of different task such as predicting the star-rating of an Amazon review based on sentiment, predicting the ontological category of a DBpedia article and categorizing news into topics. They get pretty good results, out-performing their baseline implementations of bag-of-words and word2vect. The paper is pretty cool because it demonstrates once again that we probably don't need to engineer features when using neural networks and large enough training data!",
      "title": "Text Understanding from Scratch",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://arxiv.org/pdf/1502.01710v1.pdf"
    },
    {
      "date": "2015-03-21",
      "summary": "The article experimentally explores how over-sampling a minority class affects classification performance of logistic regression, random forest and soft-margin support vector machine. Turns out that over-sampling is a really bad idea for logistic regression (logistic regression calibrates well to the distribution of the data so fudging the distribution will naturally affect performance), does not affect support vector machine (the training procedure is not strongly dependent on class distribution: performance depends on class distance from the margin - if both classes are far away from the margin, their distribution doesn't matter much) and improves performance of random forest. On the bright side: over-sampling will necessarily increase recall.",
      "title": "Does Balancing Classes Improve Classifier Performance?",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance"
    },
    {
      "date": "2015-03-21",
      "summary": "Don't always use the default loss function (e.g. Huber loss is good to not heavily penalize outliers and hinge loss is good to not penalize small mistakes). Add interaction terms to achieve non-linearity when using linear models, e.g. logistic regression. Always check for outliers! Always standardize features when using regularization. Coefficient sizes of linear models not always interpretable as feature importance, e.g. when attributes co-linear or not standardized.",
      "title": "7 common mistakes when doing Machine Learning",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.kdnuggets.com/2015/03/machine-learning-data-science-common-mistakes.html"
    },
    {
      "date": "2015-03-21",
      "summary": "The article compares using R over Python for exploratory data analysis and gives some compelling arguments in favour of R. Most interestingly, R is able to plot categorical versus numeric data out of the box!",
      "title": "The Grammar of Data Science",
      "topics": ["Data Visualization"],
      "type": "Article",
      "url": "http://technology.stitchfix.com/blog/2015/03/17/grammar-of-data-science"
    },
    {
      "date": "2015-03-21",
      "summary": "Useful comparison of a bunch of different LDA libraries, e.g. MALLET is no longer being developed and we should probably all be using GenSim due to focus on performance and ease-of-use.",
      "title": "Automatic topic-modelling with Latent Dirichlet Allocation",
      "topics": ["Natural Language Processing"],
      "type": "Article",
      "url": "http://engineering.intenthq.com/2015/02/automatic-topic-modelling-with-lda"
    },
    {
      "date": "2015-03-21",
      "summary": "Interesting essay on trying to move away from click-through-rate as the main way of evaluating the performance of recommender systems. The article argues that click-through-rate is expensive to experiment with (requires full-scale A/B test) and is a poor proxy for quality of recommendations (clicks do not necessarily imply satisfaction with the content). The article argues that we should be using human evaluation to validate new recommenders (which also allows us to get feedback while building a new algorithm, not only once we're done and productionized). Obviously, there are some issues with the approach recommended by the article: it is hard to scale human evaluation and there is no jeopardy for the participant, i.e. although they might say that they are satisfied with some recommendation, that does not necessarily translate into purchase intent given that agreeing with some recommendation does not cost the participant anything. Using human evaluation during the prototyping phase of recommender development, on the other hand, does sound appealing.",
      "title": "Moving Beyond CTR: Better Recommendations Through Human Evaluation",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://blog.echen.me/2014/10/07/moving-beyond-ctr-better-recommendations-through-human-evaluation"
    },
    {
      "date": "2015-03-04",
      "summary": "Good enumeration of different ways in which we can over-fit models, ranging from the classic 'too complex model with too little data' over to 'too finely tuned meta-parameters' to more subtle points like choosing the metric that makes our algorithm look good or choosing metrics that are inherently prone to over-fitting (like entropy, mutual information or leave-one-out cross validation).",
      "title": "Clever Methods of Overfitting",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://hunch.net/?p=22"
    },
    {
      "date": "2015-02-21",
      "summary": "Interesting diatribe porting some of the lean-agile principles to data-based products. E.g. validate product idea before investing in fancy algorithms and systems (c.f. Amazon building simple collaborative filtering to validate need for recommender systems, c.f. Using humans as initial version of a machine learning system). E.g. trade-off precall for precision when machine learning system has accountability i.e. explains what it's doing: false positives are more costly in terms of user trust. E.g. ground user's expectations: under-promise and over-deliver. E.g. use humans in the loop (c.f. LinkedIn's recommend this recommended job to a friend acts both as a buffer to not disappoint users with bad recommendations and as a way to get good training data on which jobs are relevant). E.g. engage users with the product (c.f. Give access to user's data so that they can feel in charge and clean it up). E.g. try to collect highest quality data possible (c.f. Limit number of free-text fields, pre-populate fields, etc.).",
      "title": "Data Jujitsu: The Art of Turning Data into Product",
      "topics": ["Data Science"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B008HMN5BE/"
    },
    {
      "date": "2015-02-20",
      "summary": "Much more language specific than 'Effective Java'. Most of the stuff in here would probably be of most use to advanced beginners and should be obvious to anyone with some experience (Pythonic Thinking, Functions, Classes and Inheritance, Built-in Modules, Collaboration, Production). Some of the contents I found useful (descriptors, metaclasses, concurrency),",
      "title": "Effective Python: 59 Specific Ways to Write Better Python",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B00TKGY0GU/"
    },
    {
      "date": "2015-02-18",
      "summary": "Very short paper looking at different ways to determine the language of short fragments of text. They find that using an n-gram based classifier approach they can achieve almost 100% accuracy, beating vector space models and language models. This means that we probably have some highly discriminative n-grams between languages and a lot of non informative n-grams (consistent with what we'd expect intuitively). I was able to reproduce their result that using an n-gram based classifiers solves this problem almost perfectly.",
      "title": "Comparison of Language Identification Approaches on Short, Query-Style Texts",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://www.uni-weimar.de/medien/webis/publications/papers/lipka_2010.pdf"
    },
    {
      "date": "2015-02-18",
      "summary": "Interesting paper from Google on how much work goes into using relatively simple machine learning techniques (stochastic gradient descent) in an industry setting (learning at scale, high cost of mistakes, human-in-the-loop, changing data over time). The sections on monitoring and adapting their systems in production is especially interesting. When they re-train the production model, the new model must pass some precision/recall tests on held out data before it gets promoted to production. They also monitor the distribution of the input features and output decisions to assert that they stay reasonable similarly distributed/relevant over time.",
      "title": "Detecting Adversarial Advertisements in the Wild",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37195.pdf"
    },
    {
      "date": "2015-02-18",
      "summary": "The paper compares the performance of 179 implementations of assorted classifiers on 121 open data-sets without any data pre-processing or feature engineering. They find that random-forests and SVMs with non-linear kernels have the best performance on average in binary and multi-class settings.",
      "title": "Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf"
    },
    {
      "date": "2015-02-18",
      "summary": "Taxonomy of different types of analytics stories along dimensions. Time: reporting (describing the past), surveying (explaining the present), predicting (modelling the future). Focus: what is going on, why is it going on, how to fix it. Methods: correlation story, causation story (requires experiment).",
      "title": "10 Kinds of Stories to Tell with Data",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "https://hbr.org/2014/05/10-kinds-of-stories-to-tell-with-data/"
    },
    {
      "date": "2015-02-18",
      "summary": "Mostly fluff/obvious piece about why adding a story to data analysis is important (more easily understood by stakeholders, etc.), but with one interesting point. Given that there are supposedly only 7 types of literary narratives, using stories to communicate the results of data analytics might enable us to standardize the stories we tell with the analytics to a similar set of standard pieces. This will make it easier to communicate the high level picture of the analysis.",
      "title": "Why Data Storytelling is So Important, and Why We're So Bad at It",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://deloitte.wsj.com/cio/2015/02/10/why-data-storytelling-is-so-important-and-why-were-so-bad-at-it/"
    },
    {
      "date": "2015-02-13",
      "summary": "Explanation of the underlying assumptions in some of the classic bandit algorithms and how frequent real-world experiment settings violate these (leading to worse performance than standard A/B tests). E.g. bandits assume no delay between impulse and response (real world conversion can take days). E.g. bandits assume uniform response rate (real world conversion and levers can be correlated).",
      "title": "Don't use Bandit Algorithms - they probably won't work for you",
      "topics": ["Statistics"],
      "type": "Article",
      "url": "https://www.chrisstucchio.com/blog/2015/dont_use_bandits.html"
    },
    {
      "date": "2015-02-13",
      "summary": "Some stats tricks relevant to machine learning. E.g. time-series data should be smoothed for better results. E.g. explicitly address confounding factors like inter-class variance over time.",
      "title": "10 things statistics taught us about big data analysis",
      "topics": ["Statistics"],
      "type": "Article",
      "url": "http://www.kdnuggets.com/2015/02/10-things-statistics-big-data-analysis.html"
    },
    {
      "date": "2015-02-13",
      "summary": "The article argues that a low R^2 statistic for a regression model is not a bad thing for a model if the model attributes have good p-values -- low R^2 might simply mean that there are other attributes outside of our model that explain some of the variance in the data, not that the model we have is bad. We shouldn't indiscriminately add attributes to a model just to increase R^2 (as a matter of fact, the more attributes we add the higher R^2 is likely to be) but be judicious in choosing which attributes to add in adding more attributes (domain knowledge, p-values).",
      "title": "Is A Low R-Squared Statistic A Bad Thing?",
      "topics": ["Statistics"],
      "type": "Article",
      "url": "http://www.firstclassanalytics.com/is-a-low-r-squared-statistic-a-bad-thing.html"
    },
    {
      "date": "2015-02-08",
      "summary": "Theoretical and experimental comparison of Support Vector Machines versus Logistic Regression. When the data is multi-variate, drawn from a mixture of distributions or strongly correlated SVM is better. SVM can also achieve better results with fewer features. Logistic Regression is better for some underlying distributions in the data (Poisson, Exponential, Normal).",
      "title": "Comparison between SVM and Logistic Regression: Which One is Better to Discriminate?",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://www.kurims.kyoto-u.ac.jp/EMIS/journals/RCE/V35/v35n2a03.pdf"
    },
    {
      "date": "2015-02-08",
      "summary": "Fairly one-sided comparison of logistic regression and support vector machines. The paper claims that the latter is less likely to over-fit, better handles outliers and can better handle non-linearities in the data via kernels. The paper backs the claims up by comparing the performances of the two classifiers on data-sets with less than 100 examples.",
      "title": "Support vector machines versus logistic regression: improving prospective performance in clinical decision-making",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://onlinelibrary.wiley.com/doi/10.1002/uog.2791/pdf"
    },
    {
      "date": "2015-02-04",
      "summary": "Cautionary tale about how trends in time series data can lead to strong correlations in unrelated data: if the trend is large compared to the actual data, the correlation between trends will overwhelm the non-correlation between the data. Work around: model the trend explicitly e.g. using linear regression and subtracting the fit or use differences between data points instead of raw data.",
      "title": "Avoiding a common mistake with time series",
      "topics": ["Statistics"],
      "type": "Article",
      "url": "http://svds.com/post/avoiding-common-mistake-time-series"
    },
    {
      "date": "2015-02-04",
      "summary": "Awesome task-to-algorithm decision tree made by the Scikit-learn team. Gives a nice overview of which machine learning algorithms are canonical for what sorts of problems and requirements.",
      "title": "Choosing the right estimator",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://scikit-learn.org/stable/tutorial/machine_learning_map/"
    },
    {
      "date": "2015-02-01",
      "summary": "Some interesting guidelines on how to get started tackling a new dataset. E.g., when data-dimensionality is low, use a correlation plot as a cheap way to separate noisy features from informative features and to filter out correlated features. E.g., plot learning curve to visualize difference in training versus cross-validation error. If training error is much higher than cross-validation error, we are likely over-fitting (fix by using more training data, stronger regularization or L1 regularization to force feature sparsity). If training error is very high, we are likely under-fitting i.e., using the wrong model for the data (fix by adding more features or non-linear kernels). E.g., use progressive validation when there is too much data for cross-validation: train model in batches, evaluate the model on the next batch, add the batch to the training data, re-evaluate and check how well we adapted to the new data - when we no longer improve, might as well stop training.",
      "title": "Advice for applying Machine Learning",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "https://jmetzen.github.io/2015-01-29/ml_advice.html"
    },
    {
      "date": "2015-01-25",
      "summary": "Google using deep neural nets to achieve infuriatingly good results in image description generation: 3x improvement in BLEU over the state of the art. What's the secret-sauce? They train a neural network on raw images and then feed that into a language generation neural network that predicts word N+1 in the description using the image representation generated by the image recognizer and words 1..N in the training descriptions (basically like decoding in machine translation).",
      "title": "Show and Tell: A Neural Image Caption Generator",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://arxiv.org/pdf/1411.4555v1.pdf"
    },
    {
      "date": "2015-01-19",
      "summary": "Lessons from real-world machine learning at Google. The paper mentions several anti-patterns to avoid and how to address them. E.g. entanglement/tight coupling/changing anything changes everything. It's easy to build v1 of a machine learning system, very complicated to make changes without changing the behavior of the entire model. Mitigation: metrics for all the dimensions, ensembling, better regularization. E.g. hidden feedback loops. Behavior of system affects future training data. Mitigation: ... un-solved problem. E.g. unstable data dependencies. Changes in input data sources can affect behavior of model. Mitigation: data versioning, but this is expensive in terms of engineering. E.g. underutilized features. Features with small improvement in model performance carry high cost and risk (see unstable data dependencies). Mitigation: regularly evaluate features and prune mercilessly. E.g. correction cascades. Models that use other models as features lead to coupling and make it more difficult to improve the models because of complex effects on the down-stream models. Mitigation: add the extra features of the down-stream model directly to the upstream-model. E.g. glue code. Interfacing with machine learning systems lead to glue code to adapt to the interface. Usually this means tight-coupling with particular versions. Mitigation: re-write the algorithms in the system language. E.g. monitoring. Difficult to know the correct behavior of a constantly adapting system. Mitigation: assert that observed and predicted priors are similar and threshold model confidence before taking action.",
      "title": "Machine Learning: The High-Interest Credit Card of Technical Debt",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf"
    },
    {
      "date": "2015-01-18",
      "summary": "Not sure how useful this book is. On one hand, the text offers a good introduction to some of the more widely used patterns - much more approachable than the Gang-of-Four book. On the other hand, because the book focuses on the more readily applicable patterns, I found there to be little value or news in the text.",
      "title": "Refactoring to Patterns",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/0321213351/"
    },
    {
      "date": "2015-01-18",
      "summary": "Interesting post talking about how to detect headless browsers. Most of the tips are fairly specific to Phantom JS, however, the post nicely shows the mindset to have if one truly wants to get rid of headless traffic: know and exploit the quirks in the headless rendering engines or use delay-based approaches.",
      "title": "Detecting Phantom JS-based visitors",
      "topics": ["Software Engineering"],
      "type": "Article",
      "url": "http://engineering.shapesecurity.com/2015/01/detecting-phantomjs-based-visitors.html"
    },
    {
      "date": "2014-12-07",
      "summary": "Two main approaches. 1. Use a cost sensitive algorithm i.e. encode a penalty for wrong classifications of a particular class in the objective function. 2. Use sampling to make the data-ratios more equal. Sampling is the way to go when there is no cost-sensitive version of the preferred learning algorithm or when we need to subsample the data anyway e.g. because we have too much of it. If we don't know the correct class weights, we should use area under ROC curve as objective function to find the correct sampling rate or class weights. For small data-sets, don't use under-sampling because we can't afford to reject training examples and cost-sensitive learning might be worse than over-sampling because the classifier doesn't have enough data-points to estimate the correct class weights. For large data-sets cost-sensitive learning is usually best.",
      "title": "Cost-Sensitive Learning vs. Sampling: Which is Best for Handling Unbalanced Classes with Unequal Error Costs?",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://storm.cis.fordham.edu/gweiss/papers/dmin07-weiss.pdf"
    },
    {
      "date": "2014-12-07",
      "summary": "Better than over-sampling with replacement: create artificial examples by interpolating between k-nearest-neighbours. This leads to larger and less specific decision boundaries instead of over-sampling's more specific boundaries i.e. less chance of over-fitting. Extensions of the technique to non-numeric features exist.",
      "title": "SMOTE: Synthetic Minority Over-sampling Technique",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://arxiv.org/pdf/1106.1813.pdf"
    },
    {
      "date": "2014-12-07",
      "summary": "Some useful machine learning in the wild tricks from Google: use ensemble methods, get a human into the loop on low-confidence data-points, use many features and let L1 regularization trim the space down, avoid having to classify entries by ranking them instead (easier), monitor performance of system in the wild (e.g. permanently re-compute model precision and recall).",
      "title": "Practical machine learning tricks from the KDD 2011 best industry paper",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://blog.david-andrzejewski.com/machine-learning/practical-machine-learning-tricks-from-the-kdd-2011-best-industry-paper"
    },
    {
      "date": "2014-11-30",
      "summary": "This book was a real mixed bag. On one hand, a lot of the advice in the book seemed quite technology specific and will probably be out-of-date sooner rather than later. On the other hand, there was a lot of good stuff in here about how to build robust and failure-tolerant systems and architecture. Just being aware of the patterns and failure-modes that the book talks about made me think differently about my day-to-day work. This is one to be re-visited further down the road of the software developer journey.",
      "title": "Release It!: Design and Deploy Production-Ready Software",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/0978739213/"
    },
    {
      "date": "2014-11-26",
      "summary": "Very cool NLG idea: spend November (NaNoWriMo) writing a program to generate a novel.",
      "title": "The strange world of computer-generated novels",
      "topics": ["Natural Language Processing"],
      "type": "Article",
      "url": "http://www.theverge.com/2014/11/25/7276157/nanogenmo-robot-author-novel"
    },
    {
      "date": "2014-11-25",
      "summary": "Wrapper around Matplotlib to make simple plot creation easier.",
      "title": "Seaborn: statistical data visualization",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "http://web.stanford.edu/~mwaskom/software/seaborn"
    },
    {
      "date": "2014-11-25",
      "summary": "Comparison of K-Means and DBSCAN clustering algorithms. Naturally the later wins because it is able to infer the number of clusters automatically and does not suffer from initialization problems.",
      "title": "Clustering to Reduce Spatial Data Set Size",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size"
    },
    {
      "date": "2014-11-25",
      "summary": "Comparison of discriminative sequence tagging libraries based Conditional Random Fields (allows for any state in the sequence to be considered when making the tagging decision - prone to overfitting and slow) and MaxEnt Markov Models (only considers local context when making the tagging decision - fast and less likely to overfit but weaker features). Stanford Tagger, Mallet, CRF++ and SVM tool are too slow. OpenNLP only has MEMM, CRF Suite only has CRF. OpenNLP has no multi-core support. Wapiti is the best compromise between completeness, efficiency, documentation and accuracy.",
      "title": "A review of sparse sequence taggers",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "http://fnl.es/a-review-of-sparse-sequence-taggers.html"
    },
    {
      "date": "2014-11-25",
      "summary": "Some interesting pragmatic data pre-processing ideas. E.g.: using anomaly detection to remove the need for creating a labelled data set from a classification problem and using LDA to find regular expressions that will cheaply capture some aspects of clusters in the data.",
      "title": "Using Data Science to get a Good Deal on a Macbook",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://blog.dominodatalab.com/using-data-science-to-get-a-good-deal-on-a-macbook"
    },
    {
      "date": "2014-10-25",
      "summary": "Lots of good stuff in here about low level things that make code readable. The writing is quite entertaining. Most importantly, the book has lots of examples that make it easy to relate to the concepts that the author is trying to communicate. However, most (if not all) of the stuff in here is very basic and should already be known to anyone who has been writing software professionally for any amount of time. The book probably is a great resource for someone about to start an internship, but beyond that, I question its value.",
      "title": "Clean Code: A Handbook of Agile Software Craftsmanship",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/0132350882/"
    },
    {
      "date": "2014-09-29",
      "summary": "There is a way to construct finite state automata that compute the edit distance between two strings. This means that we can compute edit distance in linear time (plus some one-time overhead to construct the automata).",
      "title": "Damn Cool Algorithms: Levenshtein Automata",
      "topics": ["Natural Language Processing"],
      "type": "Article",
      "url": "http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata"
    },
    {
      "date": "2014-09-29",
      "summary": "Lots of information retrieval functionality is built into Postgres by default: stemming, spell-checking, TF-IDF vectorization, query ranking, multi-language support, stop-wording, etc. This means we don't need to break out ElasticSearch for smaller scale full-text search.",
      "title": "Postgres full-text search is Good Enough!",
      "topics": ["Natural Language Processing"],
      "type": "Article",
      "url": "http://blog.lostpropertyhq.com/postgres-full-text-search-is-good-enough/"
    },
    {
      "date": "2014-09-28",
      "summary": "More of a hand-book for managers than a book about management. Lots of good, common sense advise about the day-to-day of building teams and running projects... But not very useful for non-manager types.",
      "title": "Peopleware: Productive Projects and Teams",
      "topics": ["Management"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B00DY5A8X2/"
    },
    {
      "date": "2014-08-29",
      "summary": "We can now enumerate the items in a Bloom filter!",
      "title": "Invertible Bloom Lookup Tables",
      "topics": ["Computer Science"],
      "type": "Paper",
      "url": "http://arxiv.org/pdf/1101.2245v2.pdf"
    },
    {
      "date": "2014-08-29",
      "summary": "Can reformulate K-Means as an (online) Map-Reduce problem.",
      "title": "Clustering data at scale",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://2013.berlinbuzzwords.de/sites/2013.berlinbuzzwords.de/files/slides/DanFilimon.pdf"
    },
    {
      "date": "2014-08-29",
      "summary": "Not as mind-blowing as on its first read through, but still lots of good stuff. The first few chapters are a good general read and reminder of some of the important things when writing software. The later chapters (concurrency, serialization, etc.) make for good reference material.",
      "title": "Effective Java: Second Edition",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B00B8V09HY/"
    },
    {
      "date": "2014-08-29",
      "summary": "It's important to have data scientists situated with the product teams - shorter feedback loops, closer to the product, can pitch in on the actual implementation, etc. For a data scientist manager it's important to pair up people with projects that interest them and make sure that there is enough mentoring or peer-learning going on.",
      "title": "What I Learned As Pandora's First Data Scientist",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://firstround.com/article/What-I-Learned-As-Pandoras-First-Data-Scientist"
    },
    {
      "date": "2014-08-28",
      "summary": "Matroid is a set of independence systems I over X such that any non-maximally independent set can be grown by adding an element from a bigger independent set. If a problem can be formulated as a Matroid then a greedy algorithm will always be optimal.",
      "title": "When Greedy Algorithms are Perfect: the Matroid",
      "topics": ["Computer Science"],
      "type": "Article",
      "url": "http://jeremykun.com/2014/08/26/when-greedy-algorithms-are-perfect-the-matroid/"
    },
    {
      "date": "2014-08-28",
      "summary": "Nuisance parameter: something that we have to estimate in order to measure the quantity we are interested in but that isn't directly relevant to the question at hand. Bayesian approach marginalizes them out, frequentist approach uses a single maximum likelihood estimate (which is probably going to be wrong - there are ways to fix this, but Bayesian formulation does it for free). Bayesian approach also better deals with outliers by accounting for them using a nuisance parameter and integrating their effect out.",
      "title": "Frequentism and Bayesianism II: When Results Differ",
      "topics": ["Statistics"],
      "type": "Article",
      "url": "http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/"
    },
    {
      "date": "2014-08-28",
      "summary": "Frequentist approach treats probabilities as frequencies of events. Bayesian approach treats probabilities as notion of certainty of an event (prior knowledge + knowledge given observations). Advantage of Bayesian approach is that there is little need to change the model for more complicated problems (e.g. multi-dimensional or correlated data). Also doesn't make an implicit Gaussian assumption about the data or residuals. Drawback: overkill for simple problems.",
      "title": "Frequentism and Bayesianism: A Practical Introduction",
      "topics": ["Statistics"],
      "type": "Article",
      "url": "http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/"
    },
    {
      "date": "2014-08-27",
      "summary": "Why does deep learning work? Distributed representation: every extra layer can represent an exponentially bigger family of functions because parameters can be reused: N parameters can be used to split 2^N input regions. Much more effective than non-distributed non-parametric learners like SVM or KNN.",
      "title": "Response to: Is Deep Learning the Final Frontier and the End of Signal Processing?",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "https://plus.google.com/+YoshuaBengio/posts/GJY53aahqS8"
    },
    {
      "date": "2014-08-26",
      "summary": "Setting up a honey-pot to collect data about 491 scams. IP address and reply-to email address that differs from sent-from address are highly indicative features of scams. Images in emails that include images hosted on external servers are uncannily effective at gathering personal information via emails.",
      "title": "Scambaiter: Understanding Targeted Nigerian Scams on Craigslist",
      "topics": ["Data Science"],
      "type": "Paper",
      "url": "http://wwwold.cs.umd.edu/~elaine/docs/scambaiter.pdf"
    },
    {
      "date": "2014-08-26",
      "summary": "Using reasoning based on ROC curves to find analytic bounds of classifier performance.",
      "title": "Why do Nigerian Scammers Say They are from Nigeria?",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://research.microsoft.com/pubs/167719/WhyFromNigeria.pdf"
    },
    {
      "date": "2014-08-26",
      "summary": "Main challenges in recommenders. Data sparsity: use dimensionality reduction (LSI, PCA) or try to do local recommendations in some cluster e.g. by using a decision tree or explicit clustering or external information (like product taxonomy). Scalability: there exists an incremental version of SVD,  if possible only compare co-rated items. Synonymy: LSI fixed this automatically or domain specific thesaurus. Shilling attacks: item-based more robust, explicitly account for it during data normalization, work with similarity residuals instead of absolute values. Memory-based collaborative filtering algorithm: find similar users (e.g. Pearson correlation or cosine similarity) and then aggregate their likes into recommendations (e.g. using voting or weighted averages). Model-based algorithm: Bayes net or clustering. Hybrids incorporating domain knowledge or extra structure (e.g. demographics) work best. Evaluation: MAE or RMSE or ROC AUC.",
      "title": "A Survey of Collaborative Filtering Techniques",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://delivery.acm.org/10.1145/1730000/1722966/p4-su.pdf"
    },
    {
      "date": "2014-08-26",
      "summary": "To-the-point comparison of bandit algorithms (epsilon-greedy, UCB1, MAB) with formulae for implementation.",
      "title": "Multi-Armed Bandit Algorithms for Computational Advertising",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.deepminds.co/optimization.php"
    },
    {
      "date": "2014-08-25",
      "summary": "Can model words as also having an operator function (interaction matrix with other words) in order to capture quantifiers like 'very' or 'not'. Increases performance on sentiment analysis task. Recursive deep learning is very good at inferring hierarchical structures. Can use Recursive neural networks for language modelling (better than n-gram models, especially with lower amounts of data). Can use strength of neural networks at learning hierarchical structures and increase performance of language model by predicting word categories given context and then words given categories (less expensive than straight words given context prediction). Lower perplexity, WER and BLEU (in machine translation)",
      "title": "Deep Learning for NLP (without Magic) - Part 8",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=eAGEYhgpOgY"
    },
    {
      "date": "2014-08-25",
      "summary": "Can use same recursive neural network used for sentence parsing for image parsing (similar structural composition going on). Recursive auto-encoder gets state of the art performance on sentiment analysis without hand-crafted features. Doesn't even necessarily need a dictionary of positive and negative words. State of the art performance on paraphrase task because model is robust to syntactical changes.",
      "title": "Deep Learning for NLP (without Magic) - Part 7",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=Wf156PweI70"
    },
    {
      "date": "2014-08-25",
      "summary": " Recursive neural networks can jointly learn word-vector representations and parse trees (syntax and semantics)- very strong to create semantic vector spaces of sentences.",
      "title": "Deep Learning for NLP (without Magic) - Part 6",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=Wf156PweI70"
    },
    {
      "date": "2014-08-25",
      "summary": "Practical stuff for working with deep learning: libraries, tips from the trenches.",
      "title": "Deep Learning for NLP (without Magic) - Part 9",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=BS05lYioGkg"
    },
    {
      "date": "2014-08-25",
      "summary": "Why does unsupervised pre-training work: places closer to better local optimum.",
      "title": "Deep Learning for NLP (without Magic) - Part 5",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=5D_ZSpSZmSk"
    },
    {
      "date": "2014-08-25",
      "summary": "This article is the source of the excellent 'machine learning framework' flowchart. The article uses the diagram to explain all the basic machine learning concepts (data cleanup, visualization, feature engineering, model selection, error analysis).",
      "title": "Predictive modeling, supervised machine learning, and pattern classification - the big picture",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://sebastianraschka.com/Articles/2014_intro_supervised_learning.html"
    },
    {
      "date": "2014-08-23",
      "summary": "Worked example of a data analytics task (outlier detection on police report data). Important first step when analyzing a data-set: understand the data collection mechanism (is it biased towards a particular type of event or time?). If dealing with categorical variables, try using domain knowledge to bin them and reduce dimensionality. Can use regression (e.g. GLM) to model if differences in occurrences of categorical values are correlated to something. Can use coefficient deviation (standard deviation normalized by mean) to see if groups are still comparable even though they have very different means.",
      "title": "Walking the Beat: Mining Seattle's Police Report Data",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://www.bayesimpact.org/blog/walking-the-beat.html"
    },
    {
      "date": "2014-08-22",
      "summary": "Problems with JavaScript for large applications: no type checking, no namespaces, no access modifiers, difficult to modularize, managing dependencies. Unified set of tools to help: Closure (library, templates and compiler). Closure compiler can strip out unneeded parts of libraries so need to track minimal set of dependencies by hand. Removes whitespace, uglifies, rewrites/optimizes code to only have minimal code needed to cover all paths that are actually optimized. Can also eliminate platform-specific code. Also does type-checking using annotations.",
      "title": "JavaScript Programming in the Large with Closure Tools",
      "topics": ["Libraries"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=M3uWx-fhjUc"
    },
    {
      "date": "2014-08-22",
      "summary": "Task: automatically finding all the influences on some painter. Unsupervised knowledge discovery task. Difficult. Solve correlated supervised task instead: style classification. Unsurprisingly, custom made high-level semantic features work better than generic low level features like SIFT.",
      "title": "Toward Automated Discovery of Artistic Influence",
      "topics": ["Computer Vision"],
      "type": "Paper",
      "url": "http://arxiv.org/pdf/1408.3218v1.pdf"
    },
    {
      "date": "2014-08-21",
      "summary": "How deep learning works expressed in NLP-maths terms. Every neuron essentially does no more than run a logistic regression. Feed them into each other recursively to get highly non-linear decision boundary - enables function approximation of very complicated functions. Deep networks need big non-linearities otherwise the deep nature won't offer benefits. Back-propagation (stochastic gradient descent) helps to make sure that lower layers learn good stuff. Unsupervised pre-training super important for performance of deep structures.",
      "title": "Deep Learning for NLP (without Magic) - Part 2",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=xPgLS3eAGO0"
    },
    {
      "date": "2014-08-21",
      "summary": "A talk on language design. Big languages are great but don't get done. Small languages take too much work to define productive units. Etc. The kicker: the talk only uses words of one syllable unless previously defined in order to make the point against small languages and for extensible languages.",
      "title": "Growing a Language",
      "topics": ["Computer Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=_ahvzDzKdB0"
    },
    {
      "date": "2014-08-21",
      "summary": "Most compelling reasons to use deep learning. Feature representation is learnt (reduced need for feature engineering over and over again for every domain/data-set). Ability to come up with distributional representations (less fragile than frequentist. Deals with curse of dimensionality) - network can learn kernel. Enables unsupervised and weight learning. Hierarchical feature representations: combinatorial sharing of statistic strength.",
      "title": "Deep Learning for NLP (without Magic) - Part 1",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=IF5tGEgRCTQ"
    },
    {
      "date": "2014-08-21",
      "summary": "Using neural network to learn word vectors (for context). Take a word in context (positive example), replace word with unrelated word to generate negative example. Create vocabulary table. This gives representation of each word. Need to learn a function to combine these to phrases. Train neural net to discriminate between positive and negative phrases previously generated.",
      "title": "Deep Learning for NLP (without Magic) - Part 3",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=6PmFSv5mg_E"
    },
    {
      "date": "2014-08-21",
      "summary": "Overview of some of the tools that ship with a default Java JDK install. Most interesting: javap (disassembler), jjs (JavaScript console), jhat (heap analysis).",
      "title": "The 6 built-in JDK tools the average developer should learn to use more",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "http://zeroturnaround.com/rebellabs/the-6-built-in-jdk-tools-the-average-developer-should-learn-to-use-more/"
    },
    {
      "date": "2014-08-20",
      "summary": "Spell checking: two main approaches. Naive way: edit distance between word and all other words in dictionary. Very expensive. Can make somewhat better by terminating edit distance computation when greater than some threshold. Peter Norvig way is faster: generate all words with edit distance less then some threshold and look them up in the dictionary. Problem: language dependent (more expensive for languages with large alphabet). Better: symmetric deleting approach. Pre-processing step: for every word in dictionary, also add all the words that can be created with at most N deletions. At lookup time, generate terms with an edit distance less than some threshold from input term and look them up in dictionary. Language independent and three orders of magnitude faster than Norvig way.",
      "title": "1000x Faster Spelling Correction algorithm",
      "topics": ["Natural Language Processing"],
      "type": "Article",
      "url": "http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/"
    },
    {
      "date": "2014-08-19",
      "summary": "Entertaining rant on lots of things. Very good Q&A at the end. Main topics: code complexity, bad management targets, programming mono-culture, etc. ``Write code that is easy to replace, not easy to extend.'' Programming as a means to an end, not a skill in isolation.",
      "title": "Programming is terrible - Lessons learned from a life wasted",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=csyL9EC0S0c"
    },
    {
      "date": "2014-08-19",
      "summary": "5-day design sprint: shorter than lean approach, less investment so easier to cut losses and throw a bad idea away. Tight deadline to get things done. Day 1: find 5 users in for user testing of idea at the end of the week, get right people for project on board. Day 2: find more than one idea, no group brainstorming but individual sketching full flows. Day 3: make good decisions using weighted voting (put stickers on parts of design people like) where core stakeholders have higher power. Day 4: battle royal between most voted ideas - mock all of them up and ready them for user testing. Day 5: run user study (data without launching). ``Learn early, learn often'' instead of ``ship early, ship often'' - ``rent before you buy.'' If there are different long-term and short-term stakeholders, use multiple design-sprint teams for different time-lines: requirements for 2 years from now or 4 years from now.",
      "title": "The design sprint: from Google Ventures to Google[X]",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=aWQUSiOZ0x8"
    },
    {
      "date": "2014-08-19",
      "summary": "Tests should tell a story, project source should be re-constructable just from the tests. Writing tests (executable specs) before code leads to simpler implementation. Easy to test code splits construction of objects and wiring up from logic (if-statements) - easier to mock. Dev-environment should facilitate testing - need to invest in tooling to make writing and running tests easy. Three kings of bugs: thinking bugs (unit testing), wiring bugs (end-to-end tests), ``looks funny'' (humans). All relatively easy to fix. Super bugs appear when concerns are mixed.",
      "title": "How to Write Clean, Testable Code",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=XcT4yYu_TTs"
    },
    {
      "date": "2014-08-19",
      "summary": "Fun and theme not correlated. Status most important motivator instead of cash. Reward early, reward often, don't go negative. Lower bar for engagement. As things become commodities, the customer loyalty becomes the goal.",
      "title": "Fun is the Future: Mastering Gamification",
      "topics": ["Marketing"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=6O1gNVeaE4g"
    },
    {
      "date": "2014-08-19",
      "summary": "JSON successful because it's at the intersection of different programming languages, not at the union: it allows to communicate things that every language has. No version number: stability over functionality.",
      "title": "The JSON Saga",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=-C-JoyNuQJs"
    },
    {
      "date": "2014-08-18",
      "summary": "Intuitive: make similar things look similar, use parallel structure, dangerous things should be ugly. Great documentation: reference docs + tutorials + quick-start. Opinionated: pick one way and stick with it (coding conventions, stick to one underlying philosophy, etc.).",
      "title": "How to Design Great APIs",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=qCdpTji8nxo"
    },
    {
      "date": "2014-08-18",
      "summary": "Covers some of the same things as ``Effective Java''. New things include the following. API process. Start with 1-page spec, keep it agile and bounce the spec off other people and then implement towards this spec to test the API proposal (not a waste of time: can use as examples in documentation). Functionality should be easy to explain - if it's hard to name it probably does too much. ``When in doubt, leave it out.'' Easier to add things to an API than to remove. What to document: side-effects, who owns objects passed into methods, preconditions/postconditions. ``Don't make the client do anything the module could do.'' Principle of least astonishment.",
      "title": "How To Design A Good API and Why it Matters",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=heh4OeB9A-c"
    },
    {
      "date": "2014-08-18",
      "summary": "Voronoi tessellations to maximise hit-area for UI elements: a lot faster than computing closest points on mouse-move as only have to have a handler on Voronoi-cell enter and exit. Consider triangles to make menu dialog disappearing less annoying: the closer a user moves to a menu item, the more resistance there is to close the menu dialog. As long as the mouse moves more right than down, keep the menu open.",
      "title": "User Interface Algorithms",
      "topics": ["Computer Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=90NsjKvz9Ns"
    },
    {
      "date": "2014-08-18",
      "summary": "Using same methods and naming conventions on all objects. Fewer methods and less code is better: easier to learn, more maintainable. Generate test-case permutations. Structured format for documentation allows for custom documentation browsers - different people want to visualise documentation in different ways. Maintain library focus - opinionated. Lots of Javascript-specific stuff. Checking user-agent for quirks most reliable and easiest.",
      "title": "Best Practices in Javascript Library Design",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=012mt05yzjc"
    },
    {
      "date": "2014-08-18",
      "summary": "Antifragility: getting better in the presence of errors. Examples: code that patches itself when encountering a bug. E.g. Netflix randomly crashes servers or increases/decreases latency to test their distributed algorithms. Need to minimize impact of errors e.g. via TDD + continuous deployment.",
      "title": "Principles of Antifragile Software",
      "topics": ["Software Engineering"],
      "type": "Paper",
      "url": "http://arxiv.org/pdf/1404.3056.pdf"
    },
    {
      "date": "2014-08-17",
      "summary": "Collaborative filtering finds similar users (e.g. using cosine similarity) and recommends items that one likes but not the other. Slow: O(M+N) where M is the number of users and N is the number of items. Can speed it up using dimensionality reduction but that hurts performance. Cluster-based recommendations split the user population into categories and generate local recommendations within the categories. Faster than collaborative filtering but still slow. Usually poor performance because clustering is not granular enough. Search-based recommendations use past interests as search queries to generate recommendations. Performs poorly for customers with large purchase histories due to difficulty of selecting the relevant items to based the query on. Item-to-item collaborative filtering finds similar items to the ones that a user likes and recommends those (e.g. by looking at items that are frequently purchased together - very sparse so easy to compute, can be computed off-line; on-line complexity is only related to user purchase history so essentially constant time).",
      "title": "Item-to-Item Collaborative Filtering",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://www.win.tue.nl/~laroyo/2L340/resources/Amazon-Recommendations.pdf"
    },
    {
      "date": "2014-08-17",
      "summary": "Gentle overview of some basic machine learning algorithms. Some interesting observations. K-means: can fit non-spherical clusters by using more sophisticated distance metrics (e.g. KL divergence) or we can run k-means with large k and then use agglomerative clustering as a post-processing step (this technique also means that we don't have to select k and aren't sensitive to initial centroid choice). Apriori algorithm for frequent-item-set inference: iteratively grow and prune item-sets to avoid combinatorial explosion. Ada boost algorithm: iteratively fit a weak algorithm to a data-set and re-weight the data by the error of the learner (i.e. in the next round we'll learn a model that prioritizes the errors of the previous one), then get overall prediction using weighted average of all learners: ensemble learning technique outperforms performance of the individual learners. K-NN: it's possible to remove most of the data-points without harming accuracy. Naive Bayes: get a non-linear decision boundary by including interaction terms between attributes.",
      "title": "Top 10 algorithms in data mining",
      "topics": ["Machine Learning"],
      "type": "Paper",
      "url": "http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf"
    },
    {
      "date": "2014-08-17",
      "summary": "Multi-treatment A/B tests: ways to compute sample size and significance (ANOVA + Tukey Test). But why would we do something complicated like this if we could just use bandit algorithms?",
      "title": "Multiple A/B/n Tests in Marketing with ANOVA and R",
      "topics": ["Statistics"],
      "type": "Article",
      "url": "http://www.marketingdistillery.com/2014/08/10/multiple-abn-tests-in-marketing-with-anova-and-r/"
    },
    {
      "date": "2014-08-17",
      "summary": "O(n) clustering algorithm that iteratively reassigns items to clusters based on distances.",
      "title": "Fast clustering algorithms for massive datasets",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.bigdatanews.com/profiles/blogs/fast-clustering-algorithms-for-massive-datasets"
    },
    {
      "date": "2014-08-16",
      "summary": "Diachronic interpretation of Bayes' Theorem (belief update): P(H_{t+1}|E) = P(H_t) * P(E|H_t)/P(E).",
      "title": "Bayesian statistics made (as) simple (as possible)",
      "topics": ["Statistics"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=bobeo5kFz1g"
    },
    {
      "date": "2014-08-16",
      "summary": "Mining version control to find trouble spots in a code-base: periodic cycles of commits are bad. Using sonar or NDepend to measure types of cohesion and other types of smells: coupling-in and coupling-out, percentage of abstract over concrete coupling, etc. Important: evolution of code metrics over time (more important than point-values, especially if tied to check-ins to see commits or team-members who reduce/increase metrics over time). Beta-cohesion (how much of the object's state does the method touch) can help in deciding whether a method should go on an object or not. Cyclomatic complexity: how many branches in the code, good idea for refactoring. Code risk metrics: ratio of tests to cyclomatic complexity and how far test are away in the call graph.",
      "title": "How to get productive in a project in 24h",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=KaLROwp-VDY"
    },
    {
      "date": "2014-08-16",
      "summary": "Effectiveness (doing the right things) vs efficiency (doing the thing right), single tasking.",
      "title": "It's not what you read, it's what you ignore",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=IWPgUn8tL8s"
    },
    {
      "date": "2014-08-16",
      "summary": "Taking inspiration from other languages and paradigms: higher order functions, contracts, actor-based.",
      "title": "Why you should talk to strangers",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=FPBVxpl8NMo"
    },
    {
      "date": "2014-08-15",
      "summary": "Importance of coding standards, programming principles and automation for readable and easily understandable code.",
      "title": "Maintainable JavaScript",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=c-kav7Tf834"
    },
    {
      "date": "2014-08-15",
      "summary": "Isometric JavaScript game engine (not canvas based).",
      "title": "Building a JavaScript-Based Game Engine for the Web",
      "topics": ["Libraries"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=_RRnyChxijA"
    },
    {
      "date": "2014-08-15",
      "summary": "Stuff to make development in JavaScript less painful: linters, unit testing libraries, testing of rendered pages using headless browsers like Zombie, avoid object orientation, amd for packaging.",
      "title": "Javascript sucks and it doesn't matter",
      "topics": ["Libraries"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=PV_cFx29Xz0"
    },
    {
      "date": "2014-08-15",
      "summary": "Augmenting LDA topic modelling with author information: opens up applications in stylometry and recommendations.",
      "title": "The Author-Topic Model for Authors and Documents",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://arxiv.org/pdf/1207.4169.pdf"
    },
    {
      "date": "2014-08-15",
      "summary": "Comparison of Unreal 4 engine and Unity.",
      "title": "An Unreal Decision",
      "topics": ["Computer Graphics"],
      "type": "Article",
      "url": "http://martiancraft.com/blog/2014/08/an-unreal-decision/"
    },
    {
      "date": "2014-08-15",
      "summary": "Classifier calibration: some classifiers output probabilities that are biased (e.g. SVM and boosted trees are conservative) - need to calibrate classifiers if we need exact probabilities. Platt's scaling: fit SVM, predic class labels, fit logistic regression using the SVM predictions as features and the true labels as targets. The probability values from the logistic regression are the calibrated SVM probabilities.",
      "title": "Calibrating a classifier with isotonic regression",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://fastml.com/calibrating-a-classifier-with-isotonic-regression/"
    },
    {
      "date": "2014-08-14",
      "summary": "Bokeh library: like D3 but without having to use JavaScript.",
      "title": "How to Create Interactive Browser Visualizations from Python with Bokeh",
      "topics": ["Data Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=hjW4gL9hioY"
    },
    {
      "date": "2014-08-14",
      "summary": "Interesting Python stuff: PyWeek (week-long game jam), Kivy (mobile apps library).",
      "title": "The Future of Python - A Choose Your Own Adventure",
      "topics": ["Software Engineering"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=d1a4Jbjc-vU"
    },
    {
      "date": "2014-08-13",
      "summary": "Lessons: features with high variance can dominate other features, read the scikits-docs ;-) e.g. useful algorithm-choice cheat-sheet and hints on how to tune algorithms, can extend scikits with custom loss function, randomize training data.",
      "title": "Machine learning the hard way -- a story about ponies",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=xeAB10QgDW8"
    },
    {
      "date": "2014-08-13",
      "summary": "Winning a Kaggle learning-to-rank challenge. Interesting parts: using collaborative filtering for search re-ranking, quality of search snippet as a feature, trying to break the class-labels in the dataset up further. Hyper-parameter tuning was important.",
      "title": "Dataiku\u2019s Solution to Yandex\u2019s Personalized Web Search Challenge",
      "topics": ["Data Science"],
      "type": "Paper",
      "url": "http://research.microsoft.com/en-us/um/people/nickcr/wscd2014/papers/wscdchallenge2014dataiku.pdf"
    },
    {
      "date": "2014-08-13",
      "summary": "Short and a pleasure to read. Most of the topics covered shouldn't come as a surprise, but the book is a good way of bringing the most important web-design considerations back into active memory.",
      "title": "Don't Make Me Think, Revisited: A Common Sense Approach to Web Usability",
      "topics": ["Design"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/dp/B00HJUBRPG/"
    },
    {
      "date": "2014-08-13",
      "summary": "Beyond collisions for hash-function evaluation: avalanche property. Good avalanche characterisitcs: if we flip bit i in the original string there should be ~50% chance that we flip bit j in the hashed string for all i and j.",
      "title": "You Don\u2019t Know Jack about Hashing",
      "topics": ["Computer Science"],
      "type": "Article",
      "url": "http://engineering.chartbeat.com/2014/08/13/you-dont-know-jack-about-hashing/"
    },
    {
      "date": "2014-08-12",
      "summary": "Practical issues when implementing neural networks: numerics, memory re-use, separate layers as much as possible (e.g. bias) for easier implementation, testing and re-use at the cost of speed.",
      "title": "convolutional learnings: things i learned by implementing convolutional neural nets",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://bbabenko.tumblr.com/post/83319141207/convolutional-learnings-things-i-learned-by"
    },
    {
      "date": "2014-08-11",
      "summary": "Semantic modelling: combine word-co-occurrence approach with word-context approach to get significantly better performance. Only consider non-zero elements of co-occurrence matrix in order to reduce noise. Not all training data is created equal: less in-domain data can outperform more out-of-domain data.",
      "title": "GloVe: Global Vectors for Word Representation",
      "topics": ["Natural Language Processing"],
      "type": "Paper",
      "url": "http://web.stanford.edu/~jpennin/papers/glove.pdf"
    },
    {
      "date": "2014-08-11",
      "summary": "Repeated significance error: stopping an A/B test early because of seeing significant results needs a lot higher actually observed significance in order not to risk getting bad result. Either commit to an experiment sample size or use approximate formulae to find the smallest possible sample size by relating minimum change to be detected and expected data variance. Or we could just avoid this problem altogether by using Bayesian methods that will give us a best estimate at any point in time.",
      "title": "How Not To Run An A/B Test",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.evanmiller.org/how-not-to-run-an-ab-test.html"
    },
    {
      "date": "2014-08-11",
      "summary": "A/B tests, bandits, etc. assume that conversion rates are constant over time. Can relax this assumptions by using Jacobi diffusion instead (assume it's a random walk that approaches a constant).",
      "title": "How to measure a changing conversion rate (with python code)",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.chrisstucchio.com/blog/2013/time_varying_conversion_rates.html"
    },
    {
      "date": "2014-08-11",
      "summary": "Incorporating expert knowledge into the bandit algorithm via non-uniform priors.",
      "title": "Bayesian Bandits - optimizing CTR with statistics",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.chrisstucchio.com/blog/2013/bayesian_bandit.html"
    },
    {
      "date": "2014-08-11",
      "summary": "UCB1 algorithm: introduce the use of confidence in bandit learning by estimating the best possible true mean of each arm and choosing the arm that maximizes this quantity.",
      "title": "Why Multi-armed Bandit algorithms are superior to A/B testing",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://www.chrisstucchio.com/blog/2012/bandit_algorithms_vs_ab.html"
    },
    {
      "date": "2014-08-11",
      "summary": "Mentions the concept of making exploration-time in bandit algorithms faster by factoring in the probability that each arm has of being the best so we don't waste time on arms that clearly have no chance of outperforming the others but still give some room to arms for which we merely don't have many observations: lack of evidence of success is not taken as evidence for failure.",
      "title": "Bayesian Bandits",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://tdunning.blogspot.com/2012/02/bayesian-bandits.html"
    },
    {
      "date": "2014-08-11",
      "summary": "Relates the Bayesian bandit to the Beta distribution, including probability update formula and shows the progression of likelihood of pull each arm over time.",
      "title": "Do A/B Testing the right way: Bayesian bandits",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://streetpickup.hubpages.com/hub/Do-AB-Testing-the-right-way-Bayesian-bandits"
    },
    {
      "date": "2014-08-11",
      "summary": "Simple description of bandit algorithms (epsilon-greedy learning) as applied to multiple-hypothesis A/B testing.",
      "title": "20 lines of code that will beat A/B testing every time",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://stevehanov.ca/blog/index.php?id=132"
    },
    {
      "date": "2014-08-11",
      "summary": "How to measure bandit performance (expected regret) and extensions to the model: learning rates to adapt to changes in world faster, hierarchies of bandits in order to try out different parameters like learning rates.",
      "title": "The Multi-Armed Bandit Problem",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://camdp.com/blogs/multi-armed-bandits"
    },
    {
      "date": "2014-08-11",
      "summary": "Blindly dropping dimensions with low PCA-eigenvalues can harm performance if the lower-valued dimensions correlate highly with the attribute to be predicted. Supervised PCA fixes this.",
      "title": "ML Counterexamples Pt.2 - Regression Post-PCA",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://camdp.com/blogs/machine-learning-counterexamples-pt2-post-pca-regr"
    },
    {
      "date": "2014-08-11",
      "summary": "Small regression coefficients don't always imply low impact but can simply indicate correlated variables. Should validate before dropping features or just use a better model like ridge regression.",
      "title": "Machine Learning Counterexamples Pt.1 - Discarding small coefficients in regression",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://camdp.com/blogs/machine-learning-counter-examples-pt1"
    },
    {
      "date": "2014-08-10",
      "summary": "API mashup to make videos searchable by extracting text-overlays from videos.",
      "title": "How I OCR Hundreds of Hours of Video",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "http://waldo.jaquith.org/blog/2011/02/ocr-video/"
    },
    {
      "date": "2014-08-10",
      "summary": "Faster one-to-many edit distance algorithm.",
      "title": "Fast and Easy Levenshtein distance using a Trie",
      "topics": ["Computer Science"],
      "type": "Article",
      "url": "http://stevehanov.ca/blog/index.php?id=114"
    },
    {
      "date": "2014-08-10",
      "summary": "Practical intro to deep learning: libraries, papers, gotchas.",
      "title": "So You Wanna Try Deep Learning?",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://snippyhollow.github.io/blog/2014/08/09/so-you-wanna-try-deep-learning/"
    },
    {
      "date": "2014-08-09",
      "summary": "Media Lovin' Toolkit = good video processing library for Python. Action recognition = optical flow + Harris corner detection + clustering.",
      "title": "An introduction to video action recognition",
      "topics": ["Computer Vision"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=fRN3nrrwngs"
    },
    {
      "date": "2014-08-09",
      "summary": "Unsupervised recommender systems. Baseline: cluster users + recommend most liked item in every cluster. Need to take exposure-to-item into consideration. Use random sub-population to get seeds in order to get rid of biases. Use bandit algorithm to find good seed items to suggest.",
      "title": "Recommender Systems - The Art and Science of Matching Items to Users",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=bRzOBGLCRbc"
    },
    {
      "date": "2014-08-09",
      "summary": "Graphs are not random. E.g. node-degree is Zipfian, can estimate number of three-way connections via eigenvalues. Can find similar laws for other quantities e.g. number of edges related to number of nodes over time, etc.",
      "title": "How to find patterns in large graphs",
      "topics": ["Computer Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=GBzoNgqF-gQ"
    },
    {
      "date": "2014-08-09",
      "summary": "SimHash for images.",
      "title": "Perceptual Hashing",
      "topics": ["Computer Vision"],
      "type": "Article",
      "url": "http://bertolami.com/index.php?engine=blog&content=posts&detail=perceptual-hashing"
    },
    {
      "date": "2014-08-08",
      "summary": "Sci-kit feature: pipeline to make working with feature extraction, transformation and model training easier.",
      "title": "Using scikit-learn Pipelines and FeatureUnions",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"
    },
    {
      "date": "2014-08-08",
      "summary": "Putting the linguistics into computational linguistics. Analysis of tweets to find dialects... But actually followed up by interpretation using domain knowledge. Refreshing to see Twitter data used in a principled way.",
      "title": "Computational Linguistics of Twitter Reveals the Existence of Global Superdialects",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://www.technologyreview.com/view/529836/computational-linguistics-of-twitter-reveals-the-existence-of-global-superdialects/"
    },
    {
      "date": "2014-08-08",
      "summary": "Pretty in-depth comparison of different fields that are often confused or overlap with data science. Good reference for people asking about data science.",
      "title": "16 analytic disciplines compared to data science",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://www.datasciencecentral.com/profiles/blogs/17-analytic-disciplines-compared"
    },
    {
      "date": "2014-08-08",
      "summary": "Graph-based discovery of related purchases.",
      "title": "Visualizing product relationships in a market Basket analysis",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://www.analyticsvidhya.com/blog/2014/08/visualizing-market-basket-analysis/"
    },
    {
      "date": "2014-08-08",
      "summary": "How different environments implement data-frames: Spark, R, Python, Badger.",
      "title": "Scaling up data frames",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://radar.oreilly.com/2014/08/scaling-up-data-frames.html"
    },
    {
      "date": "2014-08-07",
      "summary": "Engine for card/board games.",
      "title": "SourceForge Interview: A New Game Engine",
      "topics": ["Libraries"],
      "type": "Article",
      "url": "http://news.dice.com/2014/08/06/sourceforge-interview-new-game-engine/"
    },
    {
      "date": "2014-08-07",
      "summary": "Piece on the importance of dogfooding and just trying things out.",
      "title": "How Design Thinking Transformed Airbnb from a Failing Startup to a Billion Dollar Business",
      "topics": ["Management"],
      "type": "Article",
      "url": "http://firstround.com/article/How-design-thinking-transformed-Airbnb-from-failing-startup-to-billion-dollar-business"
    },
    {
      "date": "2014-08-07",
      "summary": "Target audience of research influences requisite skills: machines or humans?",
      "title": "The Question to Ask Before Hiring a Data Scientist",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://blogs.hbr.org/2014/08/the-question-to-ask-before-hiring-a-data-scientist/"
    },
    {
      "date": "2014-08-07",
      "summary": "API mashup and some very, very basic NLP analysis (stemmed ngram frequencies).",
      "title": "Sex and drugs and Rock\u2019n\u2019roll: Analysing the lyrics of the Rolling Stone 500 greatest songs of all time",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://apassant.net/2014/05/09/sex-and-drugs-and-rocknroll-analysing-the-lyrics-of-the-rolling-stone-500-greatest-songs-of-all-time/"
    },
    {
      "date": "2014-08-06",
      "summary": "Randomized sampling. Bloom filter. Log-log/hash-based cardinality estimation. Count-min sketch. Approximate databases for faster queries e.g. BlinkDB.",
      "title": "Probabilistic Data Structures & Approximate Solutions",
      "topics": ["Computer Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=yg1ZVdpCbFs"
    },
    {
      "date": "2014-08-06",
      "summary": "Using human validation in the recommender system loop.",
      "title": "Committing to Recommendation Algorithms",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=sAKTN-PeSwE"
    },
    {
      "date": "2014-08-06",
      "summary": "Need to use different sources of data in order not to be blinded by sampling biases in data analysis.",
      "title": "Algorithmic Illusions: Hidden Biases of Big Data",
      "topics": ["Data Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=irP5RCdpilc"
    },
    {
      "date": "2014-08-06",
      "summary": "Pragmatic Python/Javascript for visualization.",
      "title": "Getting it out there: Python, Javascript and Web-visualizations",
      "topics": ["Data Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=eLI6REKO7Qc"
    },
    {
      "date": "2014-08-06",
      "summary": "K-means clustering: run multiple times to approach global optimum, does not work well if variance is uneven in different dimensions (assumes spherical data), works well in presence of redundant dimensions but breaks down with noise, distance in high dimensions is not meaningful, use silhouette score for evaluation (ratio of distance to cluster center to all other centroids). Spectral clustering: looks for close-by points that are not close to other points. Construct similarity graph of points, find graph Laplacian, eigenvalue decomposition, take smallest eigenvectors and run K-means in there => increases performance.",
      "title": "Measuring Similarity & Clustering Data",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=R_fZNQM-2o4"
    },
    {
      "date": "2014-08-06",
      "summary": "Using data for good - the need for a human story that motivates data analysis in order to bring about change.",
      "title": "Storytelling in the Age of Big Data",
      "topics": ["Data Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=LMHOilUtX8o"
    },
    {
      "date": "2014-08-06",
      "summary": "Databases/repositories for real-time data synchronization.",
      "title": "Introducing Dat: If Git Were Designed For Big Data",
      "topics": ["Computer Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=FX7qSwz3SCk"
    },
    {
      "date": "2014-08-06",
      "summary": "Fluff. Need for communication of data science results to people who speak a different/business language.",
      "title": "Getting Big Benefits from Big Data",
      "topics": ["Data Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=A6qWCgGHT3k"
    },
    {
      "date": "2014-08-06",
      "summary": "Naively written-up API soup to perform and visualize sentiment analysis on Twitter.",
      "title": "Mapping Happiness With Twitter Natural Language Processing",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "https://medium.com/@JBramVB/mapping-happiness-with-twitter-natural-language-processing-ac231e70fe7"
    },
    {
      "date": "2014-08-06",
      "summary": "Interesting graph connectivity based anomaly detection approach.",
      "title": "Topological Anomaly Detection",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "http://unsupervisedlearning.wordpress.com/2014/08/04/topological-anomaly-detection/"
    },
    {
      "date": "2014-08-06",
      "summary": "Lots of great practical tricks to get the most out of machine learning techniques.",
      "title": "Reflecting back on one year of Kaggle contests",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://mlwave.com/reflecting-back-on-one-year-of-kaggle-contests/"
    },
    {
      "date": "2014-08-05",
      "summary": "Biases and pitfalls in visualization.",
      "title": "Winning Ways for Your Visualization Plays",
      "topics": ["Data Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=zQFHHHwc-9g"
    },
    {
      "date": "2014-08-05",
      "summary": "Using Bayesian Nets to model spatio-temporal behavior.",
      "title": "Measuring and Predicting Departures from Routine in Human Mobility",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=t4IYPCnC5iA"
    },
    {
      "date": "2014-08-05",
      "summary": "Better search using real-time engagement metrics and content overlap.",
      "title": "Search is not a solved problem",
      "topics": ["Computer Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=jVMWT7I3ATM"
    },
    {
      "date": "2014-08-05",
      "summary": "Motivational keynote: history, definition, current applications and future of data science.",
      "title": "ApacheCon Keynote",
      "topics": ["Data Science"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=SpBeJxIiLSs"
    },
    {
      "date": "2014-08-05",
      "summary": "Overview of different types of machine learning formulations and how to pragmatically choose between them: accuracy vs training/test speed vs interpretability vs simplicity.",
      "title": "Real-World Machine Learning on Big Data: Which Methods Should You Use?",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=P74spbN4PLE"
    },
    {
      "date": "2014-08-05",
      "summary": "Tokenization + TF-IDF + SVD to find and propagate similarities in text.",
      "title": "Text mining to correct missing CRM information",
      "topics": ["Natural Language Processing"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=3Z8Y_NCS0Tk"
    },
    {
      "date": "2014-08-05",
      "summary": "Very succinct and plainly written. Preaches nothing radically novel - if you've read The Pragmatic Programmer, The Mythical Man Month, Code Complete or other similar books you probably won't find many new ideas here. Being more to-the-point than any of the other tomes, the book would however make an excellent first foray into software craftsmanship.",
      "title": "Apprenticeship Patterns: Guidance for the Aspiring Software Craftsman",
      "topics": ["Software Engineering"],
      "type": "Book",
      "url": "http://www.amazon.co.uk/gp/product/B002RMSZ7E/"
    },
    {
      "date": "2014-08-05",
      "summary": "Pointers to common-in-the-real-world but less-well-covered-by-books machine learning topics: online learning, reinforcement learning, compression-based learning, time-series approaches, putting error bars on predictions, noisy data, feature engineering, unsupervised learning",
      "title": "Neglected machine learning ideas",
      "topics": ["Machine Learning"],
      "type": "Article",
      "url": "https://scottlocklin.wordpress.com/2014/07/22/neglected-machine-learning-ideas/"
    },
    {
      "date": "2014-08-05",
      "summary": "Problems with ORMs: too wide tables/too many attributes in class -> select * performance problems, links between classes -> foreign key join performance problems, information about object schema is replicated in database and code.",
      "title": "What ORMs have taught me: just learn SQL",
      "topics": ["Computer Science"],
      "type": "Article",
      "url": "http://wozniak.ca/what-orms-have-taught-me-just-learn-sql"
    },
    {
      "date": "2014-08-05",
      "summary": "Mostly fluff. Comparison of degree vs online courses vs bootcamp for data science education. Some good pointers to data science online courses and bootcamps.",
      "title": "How do I become a data scientist? An evaluation of 3 alternatives",
      "topics": ["Data Science"],
      "type": "Article",
      "url": "http://datascopeanalytics.com/what-we-think/2014/08/04/how-do-i-become-a-data-scientist-an-evaluation-of-3-alternatives"
    },
    {
      "date": "2014-08-04",
      "summary": "Stylometry using POS, word and character level TF-IDF 1,2,3-grams.",
      "title": "Authorship Attribution Using Python",
      "topics": ["Natural Language Processing"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=dBqyvpfHy8k"
    },
    {
      "date": "2014-08-04",
      "summary": "Interesting learning approach that uses tree-like structure in data explicitly.",
      "title": "Hierarchical Text Classification using Python (and friends)",
      "topics": ["Natural Language Processing"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=Xg8UtTgziZE"
    },
    {
      "date": "2014-08-04",
      "summary": "Great list of A/B testing gotchas.",
      "title": "Most Winning A/B Test Results are Illusory",
      "topics": ["Statistics"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=MdkHLS0FPMk"
    },
    {
      "date": "2014-08-04",
      "summary": "Basic recommender systems using distance measures and SVD.",
      "title": "Recommenders in Python",
      "topics": ["Machine Learning"],
      "type": "Video",
      "url": "https://www.youtube.com/watch?v=CHjWMpWVaTQ"
    },
    {
      "date": "2014-08-04",
      "summary": "Gentle introduction to basic data science concepts. The section on out-of-bag estimation for training of multiple classifiers on small amounts of data was interesting.",
      "title": "Applied Data Science",
      "topics": ["Data Science"],
      "type": "Book",
      "url": "http://columbia-applied-data-science.github.io/appdatasci.pdf"
    }
  ],
  "styling": {
    "navbarColor": "red lighten-2",
    "backgroundColor": "blue-grey"
  }
}
